from itertools import product

import torch
from torch.nn import BCEWithLogitsLoss, MarginRankingLoss
import torch.nn.functional as F

from data_process import PAD_Y_VAL

DEFAULT_EPS = 1e-10


def cross_softmax(y_pred, y_true, cnt=None, eps=DEFAULT_EPS):
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    mask = y_true == PAD_Y_VAL
    mask0 = y_true == 0.0
    mask1 = y_true == 1.0

    y_pred[mask] = float('0')
    # y_true[mask] = float('0')
    if torch.max(torch.max(y_pred[mask0 + mask1]), torch.max(-y_pred[mask0 + mask1])) > 20:
        y_pred[mask0 + mask1] = (y_pred[mask0 + mask1] * 20) / (
            torch.max(torch.max(y_pred[mask0 + mask1]), torch.max(-y_pred[mask0 + mask1])))
    y_pred1 = y_pred.clone()
    sigma = 0.2

    y_pred[mask1] = torch.sum(torch.exp(y_pred[mask0])) + torch.exp(y_pred[mask1]) \
                    + (torch.sum(torch.exp(y_pred[mask1])) - torch.exp(y_pred[mask1])) * sigma
    y_pred[mask1] = torch.exp(y_pred1[mask1]) / y_pred[mask1]
    y_pred[mask0] = torch.exp(y_pred1[mask0]) / torch.sum(torch.exp(y_pred1[mask0 + mask1]))

    alpha = 0.25
    gamma = 2
    ones = torch.ones_like(y_true[mask0 + mask1])
    true_ones = torch.ones_like(y_true[mask1])
    false_ones = torch.ones_like(y_true[mask0])
    if cnt is not None:
        q = cnt[mask1] / torch.sum(cnt[mask1])
    else:
        q = 1.

    p_t = y_pred + eps
    true_loss = torch.sum(alpha * q * torch.log(p_t[mask1]))
    false_loss = torch.sum((1 - alpha) * torch.pow(p_t[mask0], gamma) * torch.log(false_ones - p_t[mask0]))
    loss = true_loss + false_loss

    return -loss


def list_mle(y_pred, y_true, eps=DEFAULT_EPS, pad_value_indicator=PAD_Y_VAL):
    """
    ListMLE loss introduced in "Listwise Approach to Learning to Rank - Theory and Algorithm".

    Args:
        y_pred (torch.FloatTensor): predictions from the model, of shape [batch_size, list_size]
        y_true (torch.FloatTensor): ground truth labels, of shape [batch_size, list_size]
        eps (float): epsilon value, used for numerical stability
        pad_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1

    Returns:
        torch.Tensor: loss value
    """
    # shuffle for randomised tie resolution
    # 打乱顺序，先求出来list_size大小，并返回一个0到n-1的张量
    random_indices = torch.randperm(y_pred.shape[-1])
    # 对列打乱顺序
    y_pred_shuffled = y_pred[:, random_indices]
    y_true_shuffled = y_true[:, random_indices]
    # 沿着最后一个维度对真实值张量降序排序  先打乱顺序再排序？？
    y_true_sorted, indices = y_true_shuffled.sort(descending=True, dim=-1)
    # 按照indices索引对预测值也进行排列
    pred_sorted_by_true = y_pred_shuffled.gather(dim=1, index=indices)

    # 将其中-1的部分置为负无穷，也就是标记，预测中的
    mask = y_true_sorted == pad_value_indicator
    pred_sorted_by_true[mask] = float("-inf")

    # 获取第二个维度的最大值（一行中的最大值），并返回他们的索引
    max_pred_scores, _ = pred_sorted_by_true.max(dim=1, keepdim=True)

    # 预测中的每行都减去预测每行中的最大值
    pred_sorted_by_true_minus_max = pred_sorted_by_true - max_pred_scores

    # 先取指数，之后按照第二维度翻转，即对每一行中的值进行翻转，对行内进行累加，再翻转。
    cumsums = pred_sorted_by_true_minus_max.exp().flip(dims=[1]).cumsum(dim=1).flip(dims=[1])

    # 加上esp后再求log之后减去，减去减完最大值之后的值
    observation_loss = torch.log(cumsums + eps) - pred_sorted_by_true_minus_max

    # 将其中负一的部分置为0
    observation_loss[mask] = 0.0

    return observation_loss.sum(dim=1).mean()

def list_net(y_pred, y_true, eps=DEFAULT_EPS, pad_value_indicator=PAD_Y_VAL):
    """
    ListNet loss introduced in "Learning to Rank: From Pairwise Approach to Listwise Approach".

    Args:
        y_pred: predictions from the model, shape [batch_size, list_size]
        y_true: ground truth labels, shape [batch_size, list_size]
        eps: epsilon value, used for numerical stability
        pad_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1

    Returns:
        torch.Tensor: loss value
    """
    # y_pred = y_pred.clone()
    # y_true = y_true.clone()
    #
    # mask = y_true == pad_value_indicator
    # y_pred[mask] = float('-inf')
    # y_true[mask] = float('-inf')
    #
    # log_s_pred = F.log_softmax(y_pred, dim=1)
    # s_true = F.softmax(y_true, dim=1)
    #
    # return torch.mean(-torch.sum(s_true * log_s_pred, dim=1))
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    mask = y_true == pad_value_indicator
    y_pred[mask] = float('-inf')
    y_true[mask] = float('-inf')

    preds_smax = F.softmax(y_pred, dim=1)
    true_smax = F.softmax(y_true, dim=1)

    preds_smax = preds_smax + eps
    preds_log = torch.log(preds_smax)

    return torch.mean(-torch.sum(true_smax * preds_log, dim=1))


def approx_ndcg_loss(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PAD_Y_VAL, alpha=1.):
    """
    Loss based on approximate NDCG introduced in "A General Approximation Framework for Direct Optimization of
    Information Retrieval Measures". Please note that this method does not implement any kind of truncation.
    :param y_pred: predictions from the model, shape [batch_size, list_size]
    :param y_true: ground truth labels, shape [batch_size, list_size]
    :param eps: epsilon value, used for numerical stability
    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1
    :param alpha: score difference weight used in the sigmoid function
    :return: loss value, a torch.Tensor
    """
    device = y_pred.device
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    padded_mask = y_true == padded_value_indicator
    y_pred[padded_mask] = float("-inf")
    y_true[padded_mask] = float("-inf")

    # Here we sort the true and predicted relevancy scores.
    y_pred_sorted, indices_pred = y_pred.sort(descending=True, dim=-1)
    y_true_sorted, _ = y_true.sort(descending=True, dim=-1)

    # After sorting, we can mask out the pairs of indices (i, j) containing index of a padded element.
    true_sorted_by_preds = torch.gather(y_true, dim=1, index=indices_pred)
    true_diffs = true_sorted_by_preds[:, :, None] - true_sorted_by_preds[:, None, :]
    padded_pairs_mask = torch.isfinite(true_diffs)
    padded_pairs_mask.diagonal(dim1=-2, dim2=-1).zero_()

    # Here we clamp the -infs to get correct gains and ideal DCGs (maxDCGs)
    true_sorted_by_preds.clamp_(min=0.)
    y_true_sorted.clamp_(min=0.)

    # Here we find the gains, discounts and ideal DCGs per slate.
    pos_idxs = torch.arange(1, y_pred.shape[1] + 1).to(device)
    D = torch.log2(1. + pos_idxs.float())[None, :]
    maxDCGs = torch.sum((torch.pow(2, y_true_sorted) - 1) / D, dim=-1).clamp(min=eps)
    G = (torch.pow(2, true_sorted_by_preds) - 1) / maxDCGs[:, None]

    # Here we approximate the ranking positions according to Eqs 19-20 and later approximate NDCG (Eq 21)
    scores_diffs = (y_pred_sorted[:, :, None] - y_pred_sorted[:, None, :])
    scores_diffs[~padded_pairs_mask] = 0.
    approx_pos = 1. + torch.sum(padded_pairs_mask.float() * (torch.sigmoid(-alpha * scores_diffs).clamp(min=eps)),
                                dim=-1)
    approx_D = torch.log2(1. + approx_pos)
    approx_NDCG = torch.sum((G / approx_D), dim=-1)

    return -torch.mean(approx_NDCG)


def lambda_loss(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PAD_Y_VAL,
                weighing_scheme="lambdaRank_scheme", k=None,
                sigma=1., mu=10., reduction="mean", reduction_log="binary"):
    """
    LambdaLoss framework for LTR losses implementations,
    introduced in "The LambdaLoss Framework for Ranking Metric Optimization".
    Contains implementations of different weighing schemes corresponding to e.g. LambdaRank or RankNet.
    :param y_pred: predictions from the model, shape [batch_size, slate_length]
    :param y_true: ground truth labels, shape [batch_size, slate_length]
    :param eps: epsilon value, used for numerical stability
    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1
    :param weighing_scheme: a string corresponding to a name of one of the weighing schemes
    :param k: rank at which the loss is truncated
    :param sigma: score difference weight used in the sigmoid function
    :param mu: optional weight used in NDCGLoss2++ weighing scheme
    :param reduction: losses reduction method, could be either a sum or a mean
    :param reduction_log: logarithm variant used prior to masking and loss reduction, either binary or natural
    :return: loss value, a torch.Tensor
    """
    device = y_pred.device
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    padded_mask = y_true == padded_value_indicator
    y_pred[padded_mask] = float("-inf")
    y_true[padded_mask] = float("-inf")

    # Here we sort the true and predicted relevancy scores.
    y_pred_sorted, indices_pred = y_pred.sort(descending=True, dim=-1)
    y_true_sorted, _ = y_true.sort(descending=True, dim=-1)

    # After sorting, we can mask out the pairs of indices (i, j) containing index of a padded element.
    true_sorted_by_preds = torch.gather(y_true, dim=1, index=indices_pred)
    true_diffs = true_sorted_by_preds[:, :, None] - true_sorted_by_preds[:, None, :]
    padded_pairs_mask = torch.isfinite(true_diffs)

    if weighing_scheme != "ndcgLoss1_scheme":
        padded_pairs_mask = padded_pairs_mask & (true_diffs > 0)

    ndcg_at_k_mask = torch.zeros((y_pred.shape[1], y_pred.shape[1]),
                                 dtype=torch.uint8 if torch.__version__ < '1.2' else torch.bool, device=device)
    ndcg_at_k_mask[:k, :k] = 1

    # Here we clamp the -infs to get correct gains and ideal DCGs (maxDCGs)
    true_sorted_by_preds.clamp_(min=0.)
    y_true_sorted.clamp_(min=0.)

    # Here we find the gains, discounts and ideal DCGs per slate.
    pos_idxs = torch.arange(1, y_pred.shape[1] + 1).to(device)
    D = torch.log2(1. + pos_idxs.float())[None, :]
    maxDCGs = torch.sum(((torch.pow(2, y_true_sorted) - 1) / D)[:, :k], dim=-1).clamp(min=eps)
    G = (torch.pow(2, true_sorted_by_preds) - 1) / maxDCGs[:, None]

    # Here we apply appropriate weighing scheme - ndcgLoss1, ndcgLoss2, ndcgLoss2++ or no weights (=1.0)
    if weighing_scheme is None:
        weights = 1.
    else:
        weights = globals()[weighing_scheme](G, D, mu, true_sorted_by_preds)  # type: ignore

    # We are clamping the array entries to maintain correct backprop (log(0) and division by 0)
    scores_diffs = (y_pred_sorted[:, :, None] - y_pred_sorted[:, None, :]).clamp(min=-1e8, max=1e8)
    scores_diffs[torch.isnan(scores_diffs)] = 0.
    weighted_probas = (torch.sigmoid(sigma * scores_diffs).clamp(min=eps) ** weights).clamp(min=eps)
    if reduction_log == "natural":
        losses = torch.log(weighted_probas)
    elif reduction_log == "binary":
        losses = torch.log2(weighted_probas)
    else:
        raise ValueError("Reduction logarithm base can be either natural or binary")

    masked_losses = losses[padded_pairs_mask & ndcg_at_k_mask]
    if reduction == "sum":
        loss = -torch.sum(masked_losses)
    elif reduction == "mean":
        loss = -torch.mean(masked_losses)
    else:
        raise ValueError("Reduction method can be either sum or mean")

    return loss


def rank_net(y_pred, y_true, padded_value_indicator=PAD_Y_VAL, weight_by_diff=False, weight_by_diff_powed=False):
    """
    RankNet loss introduced in "Learning to Rank using Gradient Descent".
    :param y_pred: predictions from the model, shape [batch_size, slate_length]
    :param y_true: ground truth labels, shape [batch_size, slate_length]
    :param padded_value_indicator:
    :param weight_by_diff: flag indicating whether to weight the score differences by ground truth differences.
    :param weight_by_diff_powed: flag indicating whether to weight the score differences by the squared ground truth differences.
    :return: loss value, a torch.Tensor
    """
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    mask = y_true == padded_value_indicator
    y_pred[mask] = float('-inf')
    y_true[mask] = float('-inf')

    # here we generate every pair of indices from the range of document length in the batch
    document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))

    pairs_true = y_true[:, document_pairs_candidates]
    selected_pred = y_pred[:, document_pairs_candidates]

    # here we calculate the relative true relevance of every candidate pair
    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]
    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]

    # here we filter just the pairs that are 'positive' and did not involve a padded instance
    # we can do that since in the candidate pairs we had symetric pairs so we can stick with
    # positive ones for a simpler loss function formulation
    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))

    pred_diffs = pred_diffs[the_mask]

    weight = None
    if weight_by_diff:
        abs_diff = torch.abs(true_diffs)
        weight = abs_diff[the_mask]
    elif weight_by_diff_powed:
        true_pow_diffs = torch.pow(pairs_true[:, :, 0], 2) - torch.pow(pairs_true[:, :, 1], 2)
        abs_diff = torch.abs(true_pow_diffs)
        weight = abs_diff[the_mask]

    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know
    # whether one document is better than the other and not about the actual difference in
    # their relevancy levels
    true_diffs = (true_diffs > 0).type(torch.float32)
    true_diffs = true_diffs[the_mask]

    return BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)


def pairwise_hinge(y_pred, y_true, padded_value_indicator=PAD_Y_VAL):
    """
    RankNet loss introduced in "Learning to Rank using Gradient Descent".
    :param y_pred: predictions from the model, shape [batch_size, slate_length]
    :param y_true: ground truth labels, shape [batch_size, slate_length]
    :param padded_value_indicator:
    :return: loss value, a torch.Tensor
    """
    y_pred = y_pred.clone()
    y_true = y_true.clone()

    mask = y_true == padded_value_indicator
    y_pred[mask] = float('-inf')
    y_true[mask] = float('-inf')

    # here we generate every pair of indices from the range of document length in the batch
    document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))

    pairs_true = y_true[:, document_pairs_candidates]
    pairs_pred = y_pred[:, document_pairs_candidates]

    # here we calculate the relative true relevance of every candidate pair
    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]

    # here we filter just the pairs that are 'positive' and did not involve a padded instance
    # we can do that since in the candidate pairs we had symetric pairs so we can stick with
    # positive ones for a simpler loss function formulation
    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))

    s1 = pairs_pred[:, :, 0][the_mask]
    s2 = pairs_pred[:, :, 1][the_mask]
    target = the_mask.float()[the_mask]

    return MarginRankingLoss(margin=1, reduction='mean')(s1, s2, target)


def ndcgLoss1_scheme(G, D, *args):
    return (G / D)[:, :, None]


def ndcgLoss2_scheme(G, D, *args):
    pos_idxs = torch.arange(1, G.shape[1] + 1, device=G.device)
    delta_idxs = torch.abs(pos_idxs[:, None] - pos_idxs[None, :])
    deltas = torch.abs(torch.pow(torch.abs(D[0, delta_idxs - 1]), -1.) - torch.pow(torch.abs(D[0, delta_idxs]), -1.))
    deltas.diagonal().zero_()

    return deltas[None, :, :] * torch.abs(G[:, :, None] - G[:, None, :])


def lambdaRank_scheme(G, D, *args):
    return torch.abs(torch.pow(D[:, :, None], -1.) - torch.pow(D[:, None, :], -1.)) * torch.abs(
        G[:, :, None] - G[:, None, :])


def ndcgLoss2PP_scheme(G, D, *args):
    return args[0] * ndcgLoss2_scheme(G, D) + lambdaRank_scheme(G, D)


def rankNet_scheme(G, D, *args):
    return 1.


def rankNetWeightedByGTDiff_scheme(G, D, *args):
    return torch.abs(args[1][:, :, None] - args[1][:, None, :])


def rankNetWeightedByGTDiffPowed_scheme(G, D, *args):
    return torch.abs(torch.pow(args[1][:, :, None], 2) - torch.pow(args[1][:, None, :], 2))


